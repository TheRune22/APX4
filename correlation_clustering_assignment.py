# -*- coding: utf-8 -*-
"""Correlation clustering assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SBo5iXeQRVqfto5YUHKAGqELyBvp45zd

This colab contains code for creating a correlation clustering problem defined by two weight matrices, W_plus and W_minus. You should add code to implement an approximation algorithm using semidefinite programming and randomized rounding, as described in [Williamson and Shmoys](https://www.designofapproxalgs.com/book.pdf) section 6.4.

# Construct weight matrices
"""

# Fetch and import libraries
# !pip install picos -q
import picos as pc
import cvxopt as cvx
import cvxopt.lapack
from scipy.linalg import cholesky
import numpy as np

# Fetch data
# !wget -q https://raw.githubusercontent.com/rasmus-pagh/apx/main/data/denmark-0.6.txt -O denmark-0.6.txt
# !wget -q https://raw.githubusercontent.com/rasmus-pagh/apx/main/data/learning-0.6.txt -O learning-0.6.txt
# !wget -q https://raw.githubusercontent.com/rasmus-pagh/apx/main/data/copenhagen-0.5.txt -O copenhagen-0.5.txt

"""There are three data files containing [GloVe](https://nlp.stanford.edu/projects/glove/) vectors, whose dot products measure the similarity between words. The matrix W_plus is defined as dot products of such vectors, while W_minus is constant, equaling the average in W_plus."""


filenames = [
  # 'learning-0.6.txt',
  # 'copenhagen-0.5.txt',
  'denmark-0.6.txt'
]

for filename in filenames:
  # Read vectors and construct matrices
  with open(filename, 'r') as f:
    feature_vectors = []
    words = []
    for line in f:
      word, vector = line.split(';')
      words.append(word)
      vector = [ float(x) for x in vector.split(',') ]
      feature_vectors.append(vector)
    n = len(words)
    feature_vectors = np.array(feature_vectors)
    W_plus = np.dot(feature_vectors, np.transpose(feature_vectors))
    W_minus = np.ones(shape=(n,n)) * np.average(W_plus)

  """# Correlation clustering
  
  Here you can implement your approximation algorithm. It may be helpful to consult the [implementation](https://colab.research.google.com/drive/1Rhe0kra6mqt5VHc2uTlNzJ_JC6kpG8nA?usp=sharing)
  of an approximation algorithm for Max Cut. Your implementation must:
  - Define and solve the semidefinite programming relaxation
  - Output the upper bound on OPT given by the relaxation
  - Output the expected value of the objective function with a random 4-clustering
  - Output the value of the best 4-clustering found using randomized rounding (say, in 100 trials), and the words placed in each cluster.
  """

  problem = pc.Problem()
  X = pc.SymmetricVariable('X',(n,n))
  W_plus_const = pc.Constant('W+', W_plus)
  W_minus_const = pc.Constant('W-', W_minus)
  ones = pc.Constant('1', np.ones((n,n)))

  problem.add_constraint(pc.maindiag(X) == 1) # 1s on the main diagonal
  problem.add_constraint(X >> 0) # positive semidefinite
  problem.add_constraint(X >= 0)

  problem.set_objective('max', pc.trace(W_plus_const * X + W_minus_const * (ones - X)))
  problem.solve(solver='cvxopt')

  print(f"{filename}\nUpper bound from relaxation is {problem.value}")
  print(f"Random clustering has expected weight {(W_plus.sum() + W_minus.sum())/2}")

  V = cholesky(X.value + 1e-3 * np.identity(n))

  results = []
  repetitions = 1000

  # Perform randomized rounding many times
  for _ in range(repetitions):
    r1 = np.random.normal(size=n)
    r2 = np.random.normal(size=n)

    clusters = [set(), set(), set(), set()]
    cluster_map = {}

    # Get clusters
    for i, v in enumerate(V):
      test = (np.inner(r1, v) >= 0, np.inner(r2, v) >= 0)
      if test == (True, True):
        clusters[0].add(words[i])
        cluster_map[i] = 0
      elif test == (True, False):
        clusters[1].add(words[i])
        cluster_map[i] = 1
      elif test == (False, True):
        clusters[2].add(words[i])
        cluster_map[i] = 2
      elif test == (False, False):
        clusters[3].add(words[i])
        cluster_map[i] = 3
      else:
        assert False

    total_weight = 0

    # Compute weight of clusters
    for i in range(n):
      for j in range(n):
        if cluster_map[i] == cluster_map[j]:
          # Edge in E(S), i and j in same cluster
          total_weight += W_plus[i][j]
        else:
          # Edge in delta(S), i and j in different clusters
          total_weight += W_minus[i][j]

    results.append((total_weight, clusters))

  # Get best result
  best_result = max(results, key=lambda x: x[0])
  print(f"Best rounded result has value {best_result[0]}")
  print(f"Clusters are {best_result[1]}")
